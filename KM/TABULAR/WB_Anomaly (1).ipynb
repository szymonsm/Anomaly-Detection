{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b08ee28",
   "metadata": {},
   "source": [
    "# WB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fcd35",
   "metadata": {},
   "source": [
    "## Instalacja bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f8786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h5py\n",
      "  Downloading h5py-3.8.0-cp39-cp39-win_amd64.whl (2.6 MB)\n",
      "Collecting numpy>=1.14.5\n",
      "  Downloading numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Installing collected packages: numpy, h5py\n",
      "Successfully installed h5py-3.8.0 numpy-1.24.3\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b221f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (2.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51b2a5a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached pip-23.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.3\n",
      "    Uninstalling pip-20.2.3:\n",
      "      Successfully uninstalled pip-20.2.3\n",
      "Successfully installed pip-23.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af670fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\nowy folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\nowy folder\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install grpcio>=1.48.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f38774c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64afb0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.12.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Using cached tensorflow_intel-2.12.0-cp39-cp39-win_amd64.whl (272.8 MB)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (58.0.4)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.22.3-cp39-cp39-win_amd64.whl (420 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.8-py3-none-any.whl\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.10.0.2)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Using cached wrapt-1.14.1-cp39-cp39-win_amd64.whl (35 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.54.0-cp39-cp39-win_amd64.whl (4.1 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Collecting scipy>=1.7\n",
      "  Using cached scipy-1.10.1-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "Collecting ml-dtypes>=0.0.3\n",
      "  Using cached ml_dtypes-0.1.0-cp39-cp39-win_amd64.whl (120 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.17.3-py2.py3-none-any.whl (178 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.2-cp39-cp39-win_amd64.whl (16 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\appdata\\local\\r-miniconda\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.4)\n",
      "Installing collected packages: MarkupSafe, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, scipy, protobuf, opt-einsum, ml-dtypes, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, libclang, keras, jax, google-pasta, gast, flatbuffers, astunparse, tensorflow-intel, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.17.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 jax-0.4.8 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 opt-einsum-3.3.0 protobuf-4.22.3 scipy-1.10.1 tensorboard-2.12.2 tensorboard-data-server-0.7.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 werkzeug-2.2.3 wrapt-1.14.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f72c3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rozpakuj zip\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "input_file = 'camelyonpatch_level_2_split_test_x.h5.gz'\n",
    "output_file = 'camelyonpatch_level_2_split_test_x.h5'\n",
    "\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        \n",
    "input_file = 'camelyonpatch_level_2_split_test_y.h5.gz'\n",
    "output_file = 'camelyonpatch_level_2_split_test_y.h5'\n",
    "\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a245ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import h5py\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "import keras\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score \n",
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0759c2",
   "metadata": {},
   "source": [
    "## Przygotowanie zbioru danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c4132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja do podziału danych, ratio_1_0 = ile procent 0 w zbiorze\n",
    "np.random.seed(243)\n",
    "def split_data(x, y, train_ratio, val_ratio, test_ratio,ratio_1_0=0.85):\n",
    "    class_0_indices = np.where(y == 0)[0]\n",
    "    class_1_indices = np.where(y == 1)[0]\n",
    "    \n",
    "    total_samples = len(x)\n",
    "    class_0_size = len(class_0_indices)\n",
    "\n",
    "    class_1_size = int((class_0_size/ratio_1_0)) - class_0_size\n",
    "   \n",
    "    np.random.shuffle(class_0_indices)\n",
    "    np.random.shuffle(class_1_indices)\n",
    "\n",
    "    train_size_0 = int(class_0_size * train_ratio)\n",
    "    val_size_0 = int(class_0_size * val_ratio)\n",
    "    train_size_1 = int(class_1_size * train_ratio)\n",
    "    val_size_1 = int(class_1_size * val_ratio)\n",
    "    test_size_1 = int(class_1_size * test_ratio)\n",
    "    test_size_0 = int(class_0_size * test_ratio)\n",
    "    \n",
    "    x_train = np.concatenate([x[class_0_indices[:train_size_0]], x[class_1_indices[:train_size_1]]])\n",
    "    y_train = np.concatenate([y[class_0_indices[:train_size_0]], y[class_1_indices[:train_size_1]]])\n",
    "    x_val = np.concatenate([x[class_0_indices[train_size_0:train_size_0 + val_size_0]], x[class_1_indices[train_size_1:train_size_1 + val_size_1]]])\n",
    "    y_val = np.concatenate([y[class_0_indices[train_size_0:train_size_0 + val_size_0]], y[class_1_indices[train_size_1:train_size_1 + val_size_1]]])\n",
    "    x_test = np.concatenate([x[class_0_indices[train_size_0 + val_size_0:train_size_0 + val_size_0+test_size_0]], x[class_1_indices[train_size_1 + val_size_1:test_size_1+train_size_1 + val_size_1]]])\n",
    "    y_test = np.concatenate([y[class_0_indices[train_size_0 + val_size_0:train_size_0 + val_size_0+test_size_0]], y[class_1_indices[train_size_1 + val_size_1:test_size_1+train_size_1 + val_size_1]]])\n",
    "\n",
    "    train_indices = np.arange(len(x_train))\n",
    "    val_indices = np.arange(len(x_val))\n",
    "    test_indices = np.arange(len(x_test))\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(val_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    x_train, y_train = x_train[train_indices], y_train[train_indices]\n",
    "    x_val, y_val = x_val[val_indices], y_val[val_indices]\n",
    "    x_test, y_test = x_test[test_indices], y_test[test_indices]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "\n",
    "with h5py.File('camelyonpatch_level_2_split_test_x.h5', 'r') as f:\n",
    "    x_data = f['x'][:]\n",
    "\n",
    "with h5py.File('camelyonpatch_level_2_split_test_y.h5', 'r') as f:\n",
    "    y_data = f['y'][:]\n",
    "\n",
    "(x_train, y_train), (x_val,y_val) , (x_test, y_test) = split_data(x_data, y_data, 0.3, 0.1,0.3,0.5)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=lambda x: x / 255.,\n",
    "    width_shift_range=4,  \n",
    "    height_shift_range=4,  \n",
    "    horizontal_flip=True,  \n",
    "    vertical_flip=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a774238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (9834, 96, 96, 3)\n",
      "y_train shape: (9834, 1, 1, 1)\n",
      "x_val shape: (3278, 96, 96, 3)\n",
      "y_val shape: (3278, 1, 1, 1)\n",
      "x_test shape: (9834, 96, 96, 3)\n",
      "y_test shape: (9834, 1, 1, 1)\n",
      "Ilość obserwacji w train: 9834\n",
      "Ilość 1 w train: 4917\n",
      "Ilość obserwacji w walidacji: 3278\n",
      "Ilość 1 w walidacji: 1639\n",
      "Ilość obserwacji w test: 9834\n",
      "Ilość 1 w test: 4917\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"y_val shape:\", y_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "n_w = len(y_train)\n",
    "n_1 =y_train.sum()\n",
    "\n",
    "print(\"Ilość obserwacji w train:\", n_w)\n",
    "print(\"Ilość 1 w train:\", n_1)\n",
    "n_w = len(y_val)\n",
    "n_1 =y_val.sum()\n",
    "print(\"Ilość obserwacji w walidacji:\", n_w)\n",
    "print(\"Ilość 1 w walidacji:\", n_1)\n",
    "n_w = len(y_test)\n",
    "n_1 =y_test.sum()\n",
    "print(\"Ilość obserwacji w test:\", n_w)\n",
    "print(\"Ilość 1 w test:\", n_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84404e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0], 1))\n",
    "y_val = np.reshape(y_val, (y_val.shape[0], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d62be",
   "metadata": {},
   "source": [
    "## Trening modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b130a7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_preds = self.model.predict(x_val)\n",
    "        val_preds = val_preds.astype(int)\n",
    "        #val_preds = np.argmax(val_preds, axis=1)\n",
    "\n",
    "        val_accuracy = accuracy_score(y_val, val_preds)\n",
    "        val_precision = precision_score(y_val, val_preds)\n",
    "        val_recall = recall_score(y_val, val_preds)\n",
    "        val_f1_score = f1_score(y_val, val_preds)\n",
    "\n",
    "        print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Validation precision: {val_precision:.4f}\")\n",
    "        print(f\"Validation recall: {val_recall:.4f}\")\n",
    "        print(f\"Validation F1-score: {val_f1_score:.4f}\\n\")\n",
    "\n",
    "metrics_callback = MetricsCallback()\n",
    "\n",
    "#y_val_one_hot = keras.utils.to_categorical(y_val, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d67c50dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "103/103 [==============================] - 3s 25ms/step\n",
      "Validation accuracy: 0.7663\n",
      "Validation precision: 0.7160\n",
      "Validation recall: 0.8829\n",
      "Validation F1-score: 0.7907\n",
      "\n",
      "38/38 [==============================] - 36s 938ms/step - loss: 0.6335 - accuracy: 0.6726 - val_loss: 65.7379 - val_accuracy: 0.7599\n",
      "Epoch 2/3\n",
      "103/103 [==============================] - 3s 25ms/step\n",
      "Validation accuracy: 0.7441\n",
      "Validation precision: 0.6869\n",
      "Validation recall: 0.8969\n",
      "Validation F1-score: 0.7780\n",
      "\n",
      "38/38 [==============================] - 36s 947ms/step - loss: 0.4871 - accuracy: 0.7824 - val_loss: 73.3671 - val_accuracy: 0.7389\n",
      "Epoch 3/3\n",
      "103/103 [==============================] - 3s 26ms/step\n",
      "Validation accuracy: 0.7642\n",
      "Validation precision: 0.7123\n",
      "Validation recall: 0.8865\n",
      "Validation F1-score: 0.7899\n",
      "\n",
      "38/38 [==============================] - 36s 950ms/step - loss: 0.4824 - accuracy: 0.7807 - val_loss: 64.7252 - val_accuracy: 0.7602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23257f3ec40>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#y_train_one_hot = keras.utils.to_categorical(y_train, num_classes=2)\n",
    "\n",
    "model.fit(datagen.flow(x_train, y_train, batch_size=256),\n",
    "                    steps_per_epoch=len(x_train) // 256,\n",
    "                    epochs=3,\n",
    "\n",
    "                    callbacks=[metrics_callback],\n",
    "                    validation_data=(x_val, y_val)\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd29f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisz model\n",
    "model.save('model_sigmoid_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc34fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "#Wczytaj model\n",
    "model = load_model('model_sigmoid_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e972434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - 8s 24ms/step\n",
      "Test accuracy: 0.7673\n",
      "Test precision: 0.7129\n",
      "Test recall: 0.8951\n",
      "Test F1-score: 0.7937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_preds = model.predict(x_test)\n",
    "#val_preds = np.argmax(val_preds, axis=1)\n",
    "val_preds = val_preds.astype(int)\n",
    "\n",
    "val_accuracy = accuracy_score(y_test, val_preds)\n",
    "val_precision = precision_score(y_test, val_preds)\n",
    "val_recall = recall_score(y_test, val_preds)\n",
    "val_f1_score = f1_score(y_test, val_preds)\n",
    "\n",
    "print(f\"Test accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Test precision: {val_precision:.4f}\")\n",
    "print(f\"Test recall: {val_recall:.4f}\")\n",
    "print(f\"Test F1-score: {val_f1_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5453a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Konkurencyjny model można sprawdzić !TODO\n",
    "#image net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef841e2",
   "metadata": {},
   "source": [
    "## Stworzenie danych tabularycznych z warstw modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b62cea45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - 8s 25ms/step\n",
      "308/308 [==============================] - 7s 23ms/step\n",
      "103/103 [==============================] - 3s 25ms/step\n",
      "103/103 [==============================] - 2s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "#Wyekstrachowanie warstwy przedostatniej oraz warstwy ostatniej konwolucyjnej\n",
    "def get_features(model, data):\n",
    "    feature_extractor = keras.Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    feature_extractor1 = keras.Model(inputs=model.inputs, outputs=model.layers[-4].output)\n",
    "\n",
    "    return (feature_extractor.predict(data), feature_extractor1.predict(data))\n",
    "\n",
    "x_test_features,x_test_features_conv = get_features(model, x_test)\n",
    "x_val_features,x_val_features_conv = get_features(model, x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00bf25",
   "metadata": {},
   "source": [
    "## Anomaly Detection do poprawy jakości modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d26285bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/308 [..............................] - ETA: 7s "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22032\\2980966709.py:17: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  combined_pred = mode(np.vstack((iso_forest_pred, lof_pred, one_class_svm_pred)), axis=0)[0].flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - 7s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "# Train Isolation Forest, Local Outlier Factor, One-Class SVM na Val \n",
    "#współczynnik (0.2) zależny od błędu na modelu (u nas daje 75% acc czyli 1-0.75=0.25)\n",
    "iso_forest = IsolationForest(contamination=0.25)\n",
    "lof = LocalOutlierFactor(contamination=0.25, novelty=True)\n",
    "one_class_svm = OneClassSVM(nu=0.25)\n",
    "\n",
    "iso_forest.fit(x_test_features)\n",
    "lof.fit(x_test_features)\n",
    "one_class_svm.fit(x_test_features)\n",
    "\n",
    "#Predictions dla val\n",
    "iso_forest_pred = iso_forest.predict(x_val_features)\n",
    "lof_pred = lof.predict(x_val_features)\n",
    "one_class_svm_pred = one_class_svm.predict(x_val_features)\n",
    "\n",
    "#Połączony model voting\n",
    "combined_pred = mode(np.vstack((iso_forest_pred, lof_pred, one_class_svm_pred)), axis=0)[0].flatten()\n",
    "\n",
    "models = [iso_forest_pred, lof_pred, one_class_svm_pred, combined_pred]\n",
    "model_names = ['Isolation Forest', 'Local Outlier Factor', 'One-Class SVM', 'Combined (Voting)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78dbc4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103/103 [==============================] - 2s 24ms/step\n",
      "\n",
      "Isolation Forest Results:\n",
      "Comparison between incorrect model predictions and detected anomalies\n",
      "Recall (Wrong labels detected / All wrong labels): 0.15267175572519084\n",
      "Precision (Wrong labels detected / All anomalies detected): 0.14869888475836432\n",
      "Accuracy: 0.587248322147651\n",
      "Comparison between y_val and detected anomalies\n",
      "Recall: 0.1781574130567419\n",
      "Precision: 0.3618339529120198\n",
      "Accuracy: 0.4319707138499085\n",
      "\n",
      "\n",
      "Local Outlier Factor Results:\n",
      "Comparison between incorrect model predictions and detected anomalies\n",
      "Recall (Wrong labels detected / All wrong labels): 0.32697201017811706\n",
      "Precision (Wrong labels detected / All anomalies detected): 0.30414201183431955\n",
      "Accuracy: 0.6592434411226358\n",
      "Comparison between y_val and detected anomalies\n",
      "Recall: 0.23550945698596706\n",
      "Precision: 0.45680473372781066\n",
      "Accuracy: 0.4777303233679073\n",
      "\n",
      "\n",
      "One-Class SVM Results:\n",
      "Comparison between incorrect model predictions and detected anomalies\n",
      "Recall (Wrong labels detected / All wrong labels): 0.07506361323155217\n",
      "Precision (Wrong labels detected / All anomalies detected): 0.07440100882723834\n",
      "Accuracy: 0.5543014032946919\n",
      "Comparison between y_val and detected anomalies\n",
      "Recall: 0.18486882245271508\n",
      "Precision: 0.38209331651954603\n",
      "Accuracy: 0.4429530201342282\n",
      "\n",
      "\n",
      "Combined (Voting) Results:\n",
      "Comparison between incorrect model predictions and detected anomalies\n",
      "Recall (Wrong labels detected / All wrong labels): 0.10814249363867684\n",
      "Precision (Wrong labels detected / All anomalies detected): 0.12177650429799428\n",
      "Accuracy: 0.5991458206223307\n",
      "Comparison between y_val and detected anomalies\n",
      "Recall: 0.15985356924954242\n",
      "Precision: 0.3753581661891118\n",
      "Accuracy: 0.4469188529591214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Wyniki\n",
    "model_preds = np.round(model.predict(x_val)).flatten() \n",
    "for name, pred in zip(model_names, models):\n",
    "    print(f'\\n{name} Results:')\n",
    "\n",
    "    # Convert anomalies to binary labels (1 for normal, -1 for anomaly)\n",
    "    binary_pred = (pred == -1).astype(int)\n",
    "    binary_y_test = y_val.flatten() \n",
    "\n",
    "    incorrect_pred = model_preds != binary_y_test\n",
    "    \n",
    "    print('Comparison between incorrect model predictions and detected anomalies')\n",
    "      \n",
    "    print('Recall (Wrong labels detected / All wrong labels):', recall_score(incorrect_pred,binary_pred))\n",
    "    print('Precision (Wrong labels detected / All anomalies detected):', precision_score(incorrect_pred,binary_pred))\n",
    "    print('Accuracy:',accuracy_score(incorrect_pred,binary_pred))\n",
    "    print('Comparison between y_val and detected anomalies')\n",
    "      \n",
    "    print('Recall:', recall_score(y_val,binary_pred))\n",
    "    print('Precision:', precision_score(y_val,binary_pred))\n",
    "    print('Accuracy:',accuracy_score(y_val,binary_pred))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2dc82b",
   "metadata": {},
   "source": [
    "## Anomaly detection do rozpoznawania raka z warstwy konwolucyjnej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7587aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otrzymanie 15% 1 (raka) jeśli utworzony zbiór miał inaczej (ja stworzyłem 50/50 rak nie rak więc musiałem to odpalić)\n",
    "#x_val_features_conv\n",
    "class_1_indices = np.where(y_val == 1)[0]\n",
    "class_0_indices = np.where(y_val == 0)[0]\n",
    "\n",
    "np.random.shuffle(class_1_indices)\n",
    "val_size_1 = int(len(class_1_indices) * 0.2)\n",
    "    \n",
    "x_val_features_conv_with_ratio = np.concatenate([x_val_features_conv[class_0_indices], x_val_features_conv[class_1_indices[:val_size_1]]])\n",
    "y_val_with_ratio = np.concatenate([y_val[class_0_indices], y_val[class_1_indices[:val_size_1]]])\n",
    "val_indices = np.arange(len(x_val_features_conv_with_ratio))\n",
    "np.random.shuffle(val_indices)\n",
    "\n",
    "x_val_features_conv_with_ratio = x_val_features_conv_with_ratio[val_indices]\n",
    "y_val_with_ratio = y_val_with_ratio[val_indices]\n",
    "#x_test_features_conv\n",
    "class_1_indices = np.where(y_test == 1)[0]\n",
    "class_0_indices = np.where(y_test == 0)[0]\n",
    "\n",
    "np.random.shuffle(class_1_indices)\n",
    "test_size_1 = int(len(class_1_indices) * 0.2)\n",
    "    \n",
    "x_test_features_conv_with_ratio = np.concatenate([x_test_features_conv[class_0_indices], x_test_features_conv[class_1_indices[:test_size_1]]])\n",
    "y_test_with_ratio = np.concatenate([y_test[class_0_indices], y_test[class_1_indices[:test_size_1]]])\n",
    "test_indices = np.arange(len(x_test_features_conv_with_ratio))\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "x_test_features_conv_with_ratio = x_test_features_conv_with_ratio[test_indices]\n",
    "y_test_with_ratio = y_test_with_ratio[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "540b1179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.16661016949152543\n"
     ]
    }
   ],
   "source": [
    "print('Ratio:',y_test_with_ratio.sum()/len(y_test_with_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54236b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12720\\68301960.py:16: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  combined_pred = mode(np.vstack((iso_forest_pred, lof_pred, one_class_svm_pred)), axis=0)[0].flatten()\n"
     ]
    }
   ],
   "source": [
    "# Train Isolation Forest, Local Outlier Factor, One-Class SVM na Val \n",
    "iso_forest = IsolationForest(contamination=0.16)\n",
    "lof = LocalOutlierFactor(contamination=0.16, novelty=True)\n",
    "one_class_svm = OneClassSVM(nu=0.16)\n",
    "\n",
    "iso_forest.fit(x_test_features_conv_with_ratio)\n",
    "lof.fit(x_test_features_conv_with_ratio)\n",
    "one_class_svm.fit(x_test_features_conv_with_ratio)\n",
    "\n",
    "#Predictions dla Test\n",
    "iso_forest_pred = iso_forest.predict(x_val_features_conv_with_ratio)\n",
    "lof_pred = lof.predict(x_val_features_conv_with_ratio)\n",
    "one_class_svm_pred = one_class_svm.predict(x_val_features_conv_with_ratio)\n",
    "\n",
    "#Połączony model voting\n",
    "combined_pred = mode(np.vstack((iso_forest_pred, lof_pred, one_class_svm_pred)), axis=0)[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7783eb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Isolation Forest Results:\n",
      "Comparison between detected anomalies and y_test\n",
      "Recall: 0.04281345565749235\n",
      "Precision: 0.04590163934426229\n",
      "Accuracy: 0.6927772126144456\n",
      "\n",
      "Local Outlier Factor Results:\n",
      "Comparison between detected anomalies and y_test\n",
      "Recall: 0.024464831804281346\n",
      "Precision: 0.024691358024691357\n",
      "Accuracy: 0.6770091556459817\n",
      "\n",
      "One-Class SVM Results:\n",
      "Comparison between detected anomalies and y_test\n",
      "Recall: 0.0672782874617737\n",
      "Precision: 0.07213114754098361\n",
      "Accuracy: 0.7009155645981688\n",
      "\n",
      "Combined (Voting) Results:\n",
      "Comparison between detected anomalies and y_test\n",
      "Recall: 0.024464831804281346\n",
      "Precision: 0.03347280334728033\n",
      "Accuracy: 0.7202441505595117\n"
     ]
    }
   ],
   "source": [
    "#Wyniki\n",
    "models = [iso_forest_pred, lof_pred, one_class_svm_pred, combined_pred]\n",
    "model_names = ['Isolation Forest', 'Local Outlier Factor', 'One-Class SVM', 'Combined (Voting)']\n",
    "for name, pred in zip(model_names, models):\n",
    "    print(f'\\n{name} Results:')\n",
    "\n",
    "    # Convert anomalies to binary labels (1 for normal, -1 for anomaly)\n",
    "    binary_pred = (pred == -1).astype(int)\n",
    "    binary_y_test = y_val_with_ratio.flatten()\n",
    "    \n",
    "    print('Comparison between detected anomalies and y_test')\n",
    "      \n",
    "    print('Recall:', recall_score(binary_y_test,binary_pred))\n",
    "    print('Precision:', precision_score(binary_y_test,binary_pred))\n",
    "    print('Accuracy:',accuracy_score(binary_y_test,binary_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae188ad2",
   "metadata": {},
   "source": [
    "## Wytrenowanie Autoenkodera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38feb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo zrób lepszy autoenkoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab005e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val,y_val) , (x_test, y_test) = split_data(x_data, y_data, 0.3, 0.1,0.3,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e36a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "20/20 [==============================] - 49s 2s/step - loss: 31963.3320 - val_loss: 31321.9570\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 48s 2s/step - loss: 31952.0156 - val_loss: 31321.9570\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 48s 2s/step - loss: 31952.0156 - val_loss: 31321.9570\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 50s 3s/step - loss: 31952.0156 - val_loss: 31321.9570\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 50s 2s/step - loss: 31952.0156 - val_loss: 31321.9570\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_shape = (96, 96, 3)\n",
    "\n",
    "# Encoder\n",
    "input_img = Input(shape=input_shape)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Flatten()(x)\n",
    "encoded = Dense(30, activation='relu')(x)\n",
    "\n",
    "# Decoder\n",
    "x = Dense(24 * 24 * 64, activation='relu')(encoded)\n",
    "x = Reshape((24, 24, 64))(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=5,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_split=0.2)\n",
    "\n",
    "# Encoder model to extract features\n",
    "encoder = Model(input_img, encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c85abd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapisz\n",
    "autoencoder.save(\"autoencoder_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d80d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytaj\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# loaded_autoencoder = load_model(\"autoencoder_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaf85cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193/193 [==============================] - 12s 63ms/step\n",
      "Average MSE: 31898.174\n"
     ]
    }
   ],
   "source": [
    "#MSE jak dobry jest nasz enkoder\n",
    "\n",
    "#Odtworzone przez enkoder obrazy dla test\n",
    "reconstructed_images = autoencoder.predict(x_test)\n",
    "\n",
    "#MSE dla każdego obrazka\n",
    "mse_per_image = np.mean((x_test - reconstructed_images) ** 2, axis=(1, 2, 3))\n",
    "\n",
    "#Avg MSE\n",
    "average_mse = np.mean(mse_per_image)\n",
    "\n",
    "print(\"Average MSE:\", average_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3bef1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 26ms/step\n",
      "193/193 [==============================] - 5s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "#Encode dla x_val i x_test\n",
    "x_val_encoded = encoder.predict(x_val)\n",
    "x_test_encoded = encoder.predict(x_test)\n",
    "\n",
    "\n",
    "x_val_encoded = x_val_encoded.reshape(x_val_encoded.shape[0], -1)\n",
    "x_test_encoded = x_test_encoded.reshape(x_test_encoded.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "518a2fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneClassSVM(nu=0.16)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneClassSVM</label><div class=\"sk-toggleable__content\"><pre>OneClassSVM(nu=0.16)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneClassSVM(nu=0.16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train Isolation Forest,  Local Outlier Factor, One-Class SVM \n",
    "iso_forest_encoded = IsolationForest(contamination=0.16)\n",
    "iso_forest_encoded.fit(x_test_encoded)\n",
    "\n",
    "lof_encoded = LocalOutlierFactor(contamination=0.16, novelty=True)\n",
    "lof_encoded.fit(x_test_encoded)\n",
    "\n",
    "oc_svm_encoded = OneClassSVM(nu=0.16, kernel='rbf', gamma='scale')\n",
    "oc_svm_encoded.fit(x_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f494153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_12720\\2103271786.py:7: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  combined_pred = mode(np.vstack((iso_forest_encoded_pred, lof_encoded_pred, oc_svm_encoded_pred)), axis=0)[0].flatten()\n"
     ]
    }
   ],
   "source": [
    "#Predictions dla Test\n",
    "iso_forest_encoded_pred = iso_forest_encoded.predict(x_val_encoded)\n",
    "lof_encoded_pred = lof_encoded.predict(x_val_encoded)\n",
    "oc_svm_encoded_pred = oc_svm_encoded.predict(x_val_encoded)\n",
    "\n",
    "#Połączony model voting\n",
    "combined_pred = mode(np.vstack((iso_forest_encoded_pred, lof_encoded_pred, oc_svm_encoded_pred)), axis=0)[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193bc2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Isolation Forest Results:\n",
      "Comparison between detected anomalies and y_test\n",
      "Recall: 0.1100244498777506\n",
      "Precision: 0.1461038961038961\n",
      "Accuracy: 0.69384765625\n",
      "\n",
      "Local Outlier Factor Results:\n",
      "Comparison between detected anomalies and y_test\n",
      "Recall: 0.10757946210268948\n",
      "Precision: 0.12188365650969529\n",
      "Accuracy: 0.6669921875\n",
      "\n",
      "One-Class SVM Results:\n",
      "Comparison between detected anomalies and y_test\n",
      "Recall: 0.08801955990220049\n",
      "Precision: 0.11428571428571428\n",
      "Accuracy: 0.681640625\n",
      "\n",
      "Combined (Voting) Results:\n",
      "Comparison between detected anomalies and y_test\n",
      "Recall: 0.08801955990220049\n",
      "Precision: 0.12631578947368421\n",
      "Accuracy: 0.6962890625\n"
     ]
    }
   ],
   "source": [
    "#Wyniki\n",
    "models = [iso_forest_encoded_pred, lof_encoded_pred, oc_svm_encoded_pred, combined_pred]\n",
    "model_names = ['Isolation Forest', 'Local Outlier Factor', 'One-Class SVM', 'Combined (Voting)']\n",
    "for name, pred in zip(model_names, models):\n",
    "    print(f'\\n{name} Results:')\n",
    "\n",
    "    # Convert anomalies to binary labels (1 for normal, -1 for anomaly)\n",
    "    binary_pred = (pred == -1).astype(int)\n",
    "    binary_y_test = y_val.flatten()\n",
    "    \n",
    "    print('Comparison between detected anomalies and y_test')\n",
    "      \n",
    "    print('Recall:', recall_score(binary_y_test,binary_pred))\n",
    "    print('Precision:', precision_score(binary_y_test,binary_pred))\n",
    "    print('Accuracy:',accuracy_score(binary_y_test,binary_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
