{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f8786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in d:\\nowy folder\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in d:\\nowy folder\\lib\\site-packages (from h5py) (1.23.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\nowy folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\nowy folder\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51b2a5a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached pip-23.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 20.2.3\n",
      "    Uninstalling pip-20.2.3:\n",
      "      Successfully uninstalled pip-20.2.3\n",
      "Successfully installed pip-23.1\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af670fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\nowy folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\nowy folder\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install grpcio>=1.48.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64afb0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\nowy folder\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.3.3)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.22.3)\n",
      "Requirement already satisfied: setuptools in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.47.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Collecting keras<2.13,>=2.12.0 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\nowy folder\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\nowy folder\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in d:\\nowy folder\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in d:\\nowy folder\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.8.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.12.0->tensorflow)\n",
      "  Using cached grpcio-1.54.0-cp38-cp38-win_amd64.whl (4.1 MB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\nowy folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in d:\\nowy folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\nowy folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\nowy folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\nowy folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in d:\\nowy folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\nowy folder\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\nowy folder\\lib\\site-packages (from packaging->tensorflow-intel==2.12.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\nowy folder\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\nowy folder\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\nowy folder\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\nowy folder\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\nowy folder\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\nowy folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\nowy folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\nowy folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\nowy folder\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\nowy folder\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\nowy folder\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\nowy folder\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\nowy folder\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.0)\n",
      "Installing collected packages: keras, grpcio\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.3.1\n",
      "    Uninstalling Keras-2.3.1:\n",
      "      Successfully uninstalled Keras-2.3.1\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.47.0\n",
      "    Uninstalling grpcio-1.47.0:\n",
      "      Successfully uninstalled grpcio-1.47.0\n",
      "Successfully installed grpcio-1.54.0 keras-2.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (d:\\nowy folder\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (d:\\nowy folder\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-tools 1.47.0 requires protobuf<4.0dev,>=3.12.0, but you have protobuf 4.22.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f72c3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "input_file = 'camelyonpatch_level_2_split_test_x.h5.gz'\n",
    "output_file = 'camelyonpatch_level_2_split_test_x.h5'\n",
    "\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efed9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "input_file = 'camelyonpatch_level_2_split_test_y.h5.gz'\n",
    "output_file = 'camelyonpatch_level_2_split_test_y.h5'\n",
    "\n",
    "with gzip.open(input_file, 'rb') as f_in:\n",
    "    with open(output_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23c4132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def split_data(x, y, train_ratio, val_ratio, test_ratio):\n",
    "    assert train_ratio + val_ratio + test_ratio == 1, \"The sum of the ratios should be equal to 1\"\n",
    "\n",
    "    total_samples = len(x)\n",
    "    train_size = int(total_samples * train_ratio)\n",
    "    val_size = int(total_samples * val_ratio)\n",
    "\n",
    "    indices = np.arange(total_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "\n",
    "    x_train, y_train = x[train_indices], y[train_indices]\n",
    "    x_val, y_val = x[val_indices], y[val_indices]\n",
    "    x_test, y_test = x[test_indices], y[test_indices]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test, y_test)\n",
    "\n",
    "with h5py.File('camelyonpatch_level_2_split_test_x.h5', 'r') as f:\n",
    "    x_data = f['x'][:]\n",
    "\n",
    "with h5py.File('camelyonpatch_level_2_split_test_y.h5', 'r') as f:\n",
    "    y_data = f['y'][:]\n",
    "\n",
    "(x_train, y_train), _ , (x_test, y_test) = split_data(x_data, y_data, 0.2, 0.6, 0.2)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=lambda x: x / 255.,\n",
    "    width_shift_range=4,  # randomly shift images horizontally\n",
    "    height_shift_range=4,  # randomly shift images vertically\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=True)  # randomly flip images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a774238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (6553, 96, 96, 3)\n",
      "y_train shape: (6553, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84404e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_test = np.reshape(y_test, (y_test.shape[0], 1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d5453a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22208\\2437059189.py:20: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(datagen.flow(x_train, y_train, batch_size=256),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 22s 832ms/step - loss: 0.6765 - accuracy: 0.6400\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 20s 809ms/step - loss: 0.4946 - accuracy: 0.7732\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 21s 815ms/step - loss: 0.4811 - accuracy: 0.7778\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 21s 824ms/step - loss: 0.4687 - accuracy: 0.7820\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 20s 810ms/step - loss: 0.4397 - accuracy: 0.7994\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 21s 827ms/step - loss: 0.4320 - accuracy: 0.8023\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 21s 821ms/step - loss: 0.4260 - accuracy: 0.8009\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 21s 824ms/step - loss: 0.4395 - accuracy: 0.7959\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 21s 818ms/step - loss: 0.4277 - accuracy: 0.8059\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 22s 870ms/step - loss: 0.4361 - accuracy: 0.7942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21f311f4d30>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# Define your model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(30, activation='relu'))  # Added a new 30-neuron dense layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=256),\n",
    "                    steps_per_epoch=len(x_train) // 256,\n",
    "                    epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14d094b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 5s 27ms/step\n",
      "Comparison between model and y_train\n",
      "Accuracy: 0.7966\n",
      "F1 Score: 0.7937\n"
     ]
    }
   ],
   "source": [
    "y_train_binary = y_train.flatten()\n",
    "model_preds = np.round(model.predict(x_train)).flatten()\n",
    "accuracy = accuracy_score(y_train_binary, model_preds)\n",
    "f1 = f1_score(y_train_binary, model_preds)\n",
    "print(\"Comparison between model and y_train\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64c37149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1333"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_preds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0d80e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_labels.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d26285bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 5s 23ms/step\n",
      "\n",
      "Isolation Forest Results:\n",
      "Comparison between incorrect model predictions and detected anomalies\n",
      "Recall (Wrong labels detected / All wrong labels): 0.7862068965517242\n",
      "Precision (Wrong labels detected / All anomalies detected): 0.7829072872949256\n",
      "\n",
      "Local Outlier Factor Results:\n",
      "Comparison between incorrect model predictions and detected anomalies\n",
      "Recall (Wrong labels detected / All wrong labels): 0.8176245210727969\n",
      "Precision (Wrong labels detected / All anomalies detected): 0.7934560327198364\n",
      "\n",
      "One-Class SVM Results:\n",
      "Comparison between incorrect model predictions and detected anomalies\n",
      "Recall (Wrong labels detected / All wrong labels): 0.882183908045977\n",
      "Precision (Wrong labels detected / All anomalies detected): 0.7807731434384537\n",
      "\n",
      "Combined (Voting) Results:\n",
      "Comparison between incorrect model predictions and detected anomalies\n",
      "Recall (Wrong labels detected / All wrong labels): 0.8683908045977011\n",
      "Precision (Wrong labels detected / All anomalies detected): 0.7803408504045447\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Train Isolation Forest, Local Outlier Factor, and One-Class SVM models on the extracted features\n",
    "iso_forest = IsolationForest(contamination=0.2)\n",
    "lof = LocalOutlierFactor(contamination=0.2, novelty=True)\n",
    "one_class_svm = OneClassSVM(nu=0.1)\n",
    "\n",
    "iso_forest.fit(x_train_features)\n",
    "lof.fit(x_train_features)\n",
    "one_class_svm.fit(x_train_features)\n",
    "\n",
    "# 3. Make predictions using the three models\n",
    "iso_forest_pred = iso_forest.predict(x_train_features)\n",
    "lof_pred = lof.predict(x_train_features)\n",
    "one_class_svm_pred = one_class_svm.predict(x_train_features)\n",
    "\n",
    "# Combine the results using a voting scheme\n",
    "combined_pred = mode(np.vstack((iso_forest_pred, lof_pred, one_class_svm_pred)), axis=0)[0].flatten()\n",
    "\n",
    "# 4. Evaluate the models and the combined results\n",
    "models = [iso_forest_pred, lof_pred, one_class_svm_pred, combined_pred]\n",
    "model_names = ['Isolation Forest', 'Local Outlier Factor', 'One-Class SVM', 'Combined (Voting)']\n",
    "\n",
    "model_preds = np.round(model.predict(x_train)).flatten()\n",
    "   \n",
    "for name, pred in zip(model_names, models):\n",
    "    print(f'\\n{name} Results:')\n",
    "\n",
    "    # Convert anomalies to binary labels (1 for normal, -1 for anomaly)\n",
    "    binary_pred = (pred == 1).astype(int)\n",
    "    binary_y_train = (y_train.flatten() == 0).astype(int)\n",
    "\n",
    "    # Compare with incorrect model predictions\n",
    "    incorrect_pred = model_preds != binary_y_train\n",
    "\n",
    "    TP = np.sum((incorrect_pred == 1) & (binary_pred == 1))\n",
    "    FP = np.sum((incorrect_pred == 0) & (binary_pred == 1))\n",
    "    FN = np.sum((incorrect_pred == 1) & (binary_pred == 0))\n",
    "\n",
    "    if TP + FN != 0:\n",
    "        recall = TP / (TP + FN)\n",
    "    else:\n",
    "        recall = 0\n",
    "\n",
    "    if TP + FP != 0:\n",
    "        precision = TP / (TP + FP)\n",
    "    else:\n",
    "        precision = 0\n",
    "\n",
    "    print('Comparison between incorrect model predictions and detected anomalies')\n",
    "  \n",
    "    print('Recall (Wrong labels detected / All wrong labels):', recall)\n",
    "    print('Precision (Wrong labels detected / All anomalies detected):', precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b1e36a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 52s 2s/step - loss: 30107.1016 - val_loss: 30614.9316\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 51s 2s/step - loss: 30099.9609 - val_loss: 30614.9316\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 52s 2s/step - loss: 30099.9590 - val_loss: 30614.9316\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 52s 2s/step - loss: 30099.9551 - val_loss: 30614.9316\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 52s 2s/step - loss: 30099.9590 - val_loss: 30614.9316\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 52s 2s/step - loss: 30099.9590 - val_loss: 30614.9316\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 52s 2s/step - loss: 30099.9609 - val_loss: 30614.9316\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 52s 2s/step - loss: 30099.9609 - val_loss: 30614.9316\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 53s 3s/step - loss: 30099.9551 - val_loss: 30614.9316\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 54s 3s/step - loss: 30099.9609 - val_loss: 30614.9316\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_shape = (96, 96, 3)\n",
    "\n",
    "# Encoder\n",
    "input_img = Input(shape=input_shape)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Decoder\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Autoencoder model\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_split=0.2)\n",
    "\n",
    "# Encoder model to extract features\n",
    "encoder = Model(input_img, encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d3bef1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205/205 [==============================] - 5s 25ms/step\n",
      "205/205 [==============================] - 5s 25ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Train the Isolation Forest on encoded features\u001b[39;00m\n\u001b[0;32m     10\u001b[0m iso_forest_encoded \u001b[38;5;241m=\u001b[39m IsolationForest(contamination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43miso_forest_encoded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train the Local Outlier Factor on encoded features\u001b[39;00m\n\u001b[0;32m     14\u001b[0m lof_encoded \u001b[38;5;241m=\u001b[39m LocalOutlierFactor(contamination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py:307\u001b[0m, in \u001b[0;36mIsolationForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_samples_ \u001b[38;5;241m=\u001b[39m max_samples\n\u001b[0;32m    306\u001b[0m max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(np\u001b[38;5;241m.\u001b[39mlog2(\u001b[38;5;28mmax\u001b[39m(max_samples, \u001b[38;5;241m2\u001b[39m))))\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontamination \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    312\u001b[0m     \u001b[38;5;66;03m# 0.5 plays a special role as described in the original paper.\u001b[39;00m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;66;03m# we take the opposite as we consider the opposite of their score.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:394\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[0;32m    391\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[1;32m--> 394\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    412\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[0;32m    413\u001b[0m )\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Nowy folder\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:123\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose)\u001b[0m\n\u001b[0;32m    120\u001b[0m         not_indices_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m~\u001b[39mindices_to_mask(indices, n_samples)\n\u001b[0;32m    121\u001b[0m         curr_sample_weight[not_indices_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 123\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m, y, sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit((X[indices])[:, features], y[indices])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Encode the train and test datasets\n",
    "x_train_encoded = encoder.predict(x_train)\n",
    "x_test_encoded = encoder.predict(x_test)\n",
    "\n",
    "# Flatten the encoded representations\n",
    "x_train_encoded = x_train_encoded.reshape(x_train_encoded.shape[0], -1)\n",
    "x_test_encoded = x_test_encoded.reshape(x_test_encoded.shape[0], -1)\n",
    "\n",
    "# Train the Isolation Forest on encoded features\n",
    "iso_forest_encoded = IsolationForest(contamination=0.1)\n",
    "iso_forest_encoded.fit(x_train_encoded)\n",
    "\n",
    "# Train the Local Outlier Factor on encoded features\n",
    "lof_encoded = LocalOutlierFactor(contamination=0.1)\n",
    "lof_encoded.fit(x_train_encoded)\n",
    "\n",
    "# Train the One-Class SVM on encoded features\n",
    "oc_svm_encoded = OneClassSVM(nu=0.1, kernel='rbf', gamma='scale')\n",
    "oc_svm_encoded.fit(x_train_encoded)\n",
    "\n",
    "# Define a function to compare results\n",
    "def compare_results(test_data, test_labels, anomaly_model):\n",
    "    pred = anomaly_model.predict(test_data)\n",
    "    binary_pred = (pred == 1).astype(int)\n",
    "    binary_labels = (test_labels == 0).astype(int).flatten()\n",
    "    incorrect_pred = (model.predict(test_data).round().flatten() != binary_labels).astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(binary_labels, binary_pred)\n",
    "    f1 = f1_score(binary_labels, binary_pred)\n",
    "    detection_rate = np.sum(incorrect_pred * binary_pred) / np.sum(incorrect_pred)\n",
    "    false_positive_rate = np.sum(incorrect_pred * binary_pred) / np.sum(binary_pred)\n",
    "    \n",
    "    return accuracy, f1, detection_rate, false_positive_rate\n",
    "\n",
    "# Compare results for each anomaly detection algorithm\n",
    "for name, anomaly_model in [('Isolation Forest', iso_forest_encoded),\n",
    "                            ('Local Outlier Factor', lof_encoded),\n",
    "                            ('One-Class SVM', oc_svm_encoded)]:\n",
    "    accuracy, f1, detection_rate, false_positive_rate = compare_results(x_test_encoded, y_test, anomaly_model)\n",
    "    print(f\"{name}\\nAccuracy: {accuracy:.4f}\\nF1 Score: {f1:.4f}\\nDetection Rate: {detection_rate:.4f}\\nFalse Positive Rate: {false_positive_rate:.4f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
